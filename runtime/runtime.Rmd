---
title: "Supplementary File 2: Runtime performance"
author: "Zuguang Gu ( z.gu@dkfz.de )"
date: '`r Sys.Date()`'
output: 
  html_document:
    css: main.css
    toc: true
    fig_caption: true
---

```{r, echo = FALSE, message = FALSE}
library(knitr)
knitr::opts_chunk$set(
    error = FALSE,
    tidy  = FALSE,
    message = FALSE,
    warning = FALSE,
    fig.width = 6, fig.height = 6,
    fig.align = "center")
```

In this document, we benchmark the runtime performance of **simona** on ontologies
with various sizes (i.e., on the level of 1K, 10K, 100K, 1M).

```{r}
library(simona)
set.seed(123)
```

```{r, echo = FALSE}
simona_opt$verbose = FALSE
```

We write a small function which applies the `"Sim_WP_1994"` similarity method.
`"Sim_WP_1994"` is based on the DAG structure where it uses the longest
distance from root to the lowest common ancestor (LCA) term (i.e. the depth of
LCA) and the longest distance from the LCA term to the two terms.


Denote two terms as $a$ and $b$, their LCA term as $c$, $\delta(c)$ is the depth
of $c$ in the DAG, i.e. the longest distance from the root term, $\mathrm{len}(c, a)$
is the longest distance from $c$ to $a$, the `"Sim_WP_1994"` similarity is calculated as:

$$
 \mathrm{Sim}(a, b) =  \frac{2*\delta(c)}{\mathrm{len}(c, a) + \mathrm{len}(c, b) + 2*\delta(c)}
$$


```{r}
benchmark_runtime = function(dag, by = 200, max = 10000) {
    invisible(dag_depth(dag))  # depth will be cached

    n_terms = dag_n_terms(dag)
    k = seq(by, min(max, floor(n_terms/by)*by), by = by)
    t = rep(NA_real_, length(k))
    for(i in seq_along(k)) {
        message(k[i], "/", max(k), "...")
        terms = sample(n_terms, k[i]) # numeric indicies are also allowed
        t[i] = system.time(term_sim(dag, terms, method = "Sim_WP_1994"))[3]
    }
    data.frame(k = k, t = t)
}
```

In the common settings when we import ontology datasets, we set `remove_rings
= TRUE` to remove rings and `remove_cyclic_paths = TRUE` to remove cyclic
links.

## Pathway Ontology

The `pw.owl` file is downloaded from http://obofoundry.org/ontology/pw.html.
It contains several thousands of terms.

```{r}
dag = import_owl("~/workspace/ontology/OBOFoundry/pw/pw.owl", 
    remove_rings = TRUE, remove_cyclic_paths = TRUE)
dag
```

In the plot, a smooth line by loess fit is added.

```{r, eval = !file.exists("runtime_pw.rds")}
df = benchmark_runtime(dag, by = 100)
plot(df$k, df$t, xlab = "Numbers of random terms", ylab = "runtime (sec)", 
    main = paste0("Pathway Ontology, ", dag_n_terms(dag), " terms"))
x = c(0, df$k)
y = c(0, df$t)
fit = loess(y ~ x, span = 0.5)
lines(x, predict(fit))
```


```{r, echo = FALSE, fig.cap = "Figure S2-1. Runtime performance of simona on the Pathway Ontology."}
if(!file.exists("runtime_pw.rds")) {
    saveRDS(df, file = "runtime_pw.rds")
} else {
    df = readRDS("runtime_pw.rds")
}
plot(df$k, df$t, xlab = "Numbers of random terms", ylab = "runtime (sec)", 
    main = paste0("Pathway Ontology, ", dag_n_terms(dag), " terms"))
x = c(0, df$k)
y = c(0, df$t)
fit = loess(y ~ x, span = 0.5)
lines(x, predict(fit))
```

## Gene Ontology

Ontology is directly from the **GO.db** package. We use the Biological Process
(BP) namespace. To be consistent to other ontologies under test, we only take
the `"is_a"` relation type. It contains several tens of thousands of terms.

```{r}
dag = create_ontology_DAG_from_GO_db(relations = NULL)
dag
```

```{r, eval = !file.exists("runtime_gobp.rds")}
df = benchmark_runtime(dag, by = 400, max = 20000)
plot(df$k, df$t, xlab = "Numbers of random terms", ylab = "runtime (sec)", 
    main = paste0("Gene Ontology (BP), ", dag_n_terms(dag), " terms"))
x = c(0, df$k)
y = c(0, df$t)
fit = loess(y ~ x, span = 0.5)
lines(x, predict(fit))
```


```{r, echo = FALSE, fig.cap = "Figure S2-2. Runtime performance of simona on the Gene Ontology, biological process namespace."}
if(!file.exists("runtime_gobp.rds")) {
    saveRDS(df, file = "runtime_gobp.rds")
} else {
    df = readRDS("runtime_gobp.rds")
}
plot(df$k, df$t, xlab = "Numbers of random terms", ylab = "runtime (sec)", 
    main = paste0("Gene Ontology (BP), ", dag_n_terms(dag), " terms"))
x = c(0, df$k)
y = c(0, df$t)
fit = loess(y ~ x, span = 0.5)
lines(x, predict(fit))
```


##  Chemical Entities of Biological Interest

The `chebi.obo` file is dowloaded from http://obofoundry.org/ontology/chebi.html. It contains several hundreds of thousands of terms.

```{r, eval = F}
dag = import_obo("~/workspace/ontology/OBOFoundry/chebi/chebi.obo",
    remove_rings = TRUE, remove_cyclic_paths = TRUE)
dag
```

```{r, eval = !file.exists("runtime_chebi.rds")}
df = benchmark_runtime(dag, by = 400, max = 20000)
plot(df$k, df$t, xlab = "Numbers of random terms", ylab = "runtime (sec)", 
    main = paste0("Chemical Entities of Biological Interest, ", dag_n_terms(dag), " terms"))
x = c(0, df$k)
y = c(0, df$t)
fit = loess(y ~ x, span = 0.5)
lines(x, predict(fit))
```

```{r, echo = FALSE, fig.cap = "Figure S2-3. Runtime performance of simona on the Chemical Entities of Biological Interest."}
if(!file.exists("runtime_chebi.rds")) {
    saveRDS(df, file = "runtime_chebi.rds")
} else {
    df = readRDS("runtime_chebi.rds")
}
plot(df$k, df$t, xlab = "Numbers of random terms", ylab = "runtime (sec)", 
    main = paste0("Chemical Entities of Biological Interest, ", dag_n_terms(dag), " terms"))
x = c(0, df$k)
y = c(0, df$t)
fit = loess(y ~ x, span = 0.5)
lines(x, predict(fit))
```


## NCBI organismal classification

The `ncbitaxon.owl` file is downloaded from http://obofoundry.org/ontology/ncbitaxon.html. It contains several millions of terms.

```{r import_ncbitaxon, eval = F}
dag = import_owl("~/workspace/ontology/OBOFoundry/ncbitaxon/ncbitaxon.owl",
    remove_rings = TRUE, remove_cyclic_paths = TRUE)
dag
```

```{r, eval = !file.exists("runtime_ncbitaxon.rds")}
df = benchmark_runtime(dag, by = 400, max = 20000)
plot(df$k, df$t, xlab = "Numbers of random terms", ylab = "runtime (sec)", 
    main = paste0("NCBI organismal classification, ", dag_n_terms(dag), " terms"))
x = c(0, df$k)
y = c(0, df$t)
fit = loess(y ~ x, span = 0.5)
lines(x, predict(fit))
```


```{r, echo = FALSE, fig.cap = "Figure S2-4. Runtime performance of simona on the NCBI organismal classification."}
if(!file.exists("runtime_ncbitaxon.rds")) {
    saveRDS(df, file = "runtime_ncbitaxon.rds")
} else {
    df = readRDS("runtime_ncbitaxon.rds")
}
plot(df$k, df$t, xlab = "Numbers of random terms", ylab = "runtime (sec)", 
    main = paste0("NCBI organismal classification, ", dag_n_terms(dag), " terms"))
x = c(0, df$k)
y = c(0, df$t)
fit = loess(y ~ x, span = 0.5)
lines(x, predict(fit))
```

## Benchmark on OBO Foundry ontologies

We benchmark all OBO Foundry ontologies with numbers of terms larger
than 1000. We set 50 different
numbers of random terms to test, from 100 to `min(20000, dag_n_terms(dag))`. Here the
result is already generated and saved in `"runtime_OBOFoundry_all.RData"`. The
script for generating this file is
[run_time_OBOFoundry.R](run_time_OBOFoundry.R). The plots for individual ontologies
can be found in the ["OBO Foundry gallery"](../OBOFoundry_gallery/OBOFoundry_viz.html) document.

There are two objects in `"runtime_OBOFoundry_all.RData"`: 

- `lt`: a list of two-column data frames which contain numbers of random terms to test
and corresponding runtime.
- `df`: a data frame that contains meta-information of each ontology.

```{r}
load("runtime_OBOFoundry_all.RData")
length(lt)
```

```{r, echo = FALSE}
lt = lapply(lt, function(x) x[, 1:2])
```

```{r}
head(names(lt))
head(lt[[1]])
head(df)
```

In this section, we benchmark the runtime performance of **simona** from two 
apsects: on the number of query terms, and on the size of ontologies.

### On the number of query terms

Different ontologies have different ranges of runtime. To make them comparable,
we scale values on x-axis (i.e. numbers of terms) and values on y-axis
(runtime) both into the range of `[0, 1]`. In the next plot, we also add
lines for the linear, quadratic and cubic time complexities.

```{r, fig.cap = "Figure S2-5. Compare runtime performance on various ontologies. Runtime performance is scaled into [0, 1] in the plot."}
plot(NULL, xlim = c(0, 1), ylim = c(0, 1),
    xlab = "Numbers of random terms, scaled", ylab = "runtime, scaled",
    main = "Compare runtime performance of OBO Foundry ontologies")
for(i in seq_along(lt)) {
    x = lt[[i]]$k
    y = lt[[i]]$t
    x = x/max(x)
    y = y/max(y)
    lines(x, y, col = "#00000080")
}
curve(x^1, from = 0, to = 1, col = 2, lty = 2, add = TRUE)
curve(x^2, from = 0, to = 1, col = 3, lty = 2, add = TRUE)
curve(x^3, from = 0, to = 1, col = 4, lty = 2, add = TRUE)
legend("topleft", lty = 2, col = 2:4, legend = c("O(n)", "O(n^2)", "O(n^3)"))
```

In the plot, if a curve bends more to the bottom right of the plotting region,
it means the time complexity is worse. The plot shows for most of the
ontologies, **simona** has a non-linear time complexity for calculating
similarities of $n$ query terms, close to $O(n^2)$, but there are a few ontologies, on which
**simona** shows a nearly-linear time complexity.

Since the previous plot scales values on both x-axis and y-axis into `[0, 1]`,
we can measure the difference to the linear time complexity by calculating the
area difference of the curve to `y = x` (the diagonal line, the red line). If the curve bends
more to the bottom left of the plotting region, it means the it has worse time complexity.
The following function `rel_diff()` calculates such relative time complexity difference.
The third argument `max_x` is used for setting the maximal number of random terms in 
the benchmark procedures, which will be used later.

```{r}
rel_diff = function(x, y, max_x = NULL) {
    if(missing(y)) {
        y = x[[2]]
        x = x[[1]]
    }

    if(!is.null(max_x)) {
        l = x <= max_x
        x = x[l]
        y = y[l]
    }

    od = order(x)
    x = x[od]
    y = y[od]
    n = length(x)

    x = x/max(x)
    y = y/max(y)

    area = sum( 0.5*(x[2:n] - x[2:n - 1])*(y[2:n] + y[2:n - 1]) )

    0.5 - area
}
```

We find the time complexity on different ontologies is quite stable around $O(n^2)$, but
for some large ontologies, the time complexity decreases close to linear.

```{r, fig.cap = "Figure S2-6. Relative time complexity to linear complexity. Ontologies with more than 100k terms are highlighted by their names."}
df$rel_diff = sapply(lt, rel_diff)

plot(df$n_terms, df$rel_diff, log = "x",
    xlab = "Number of terms", 
    ylab = "Relative difference to linear time complexity")
abline(h = 0, lty = 2, col = 2)
abline(h = 1/6, lty = 2, col = 3) # area under quadratic curve is 1/3
abline(h = 1/4, lty = 2, col = 4) # area under cubic curve is 1/4
l = df$n_terms > 100000
text(df$n_terms[l], df$rel_diff[l], df$id[l], 
    adj = c(1, -0.4), col = "blue", cex = 0.8)
legend("topright", lty = 2, col = 2:4, legend = c("O(n)", "O(n^2)", "O(n^3)"))
```

However, this does not mean large ontologies have better time complexity than
smaller ones. Since in the benchmark procedures, the possible maximal number
of random terms to pick (let's call it `max_pick`) is set to 20000, with large
ontologies whose numbers of terms far larger than 20000, the number of picked
terms is only a tiny fraction of their total terms, thus the close-quadratic
complexity is not obviously observable. Or in other words, when the number of
terms to be picked is far smaller than the total number of terms in the ontology, 
the time complexity is close to linear.

We can validate it by setting `max_pick` to small values such as 5000 and 1000, shown in the following plots.
When `max_pick` becomes smaller, such reduction to linear complexity can be observed in more ontologies.

```{r, echo = FALSE, fig.width = 10, fig.height = 10, fig.cap = "Figure S2-7. Compare the effect of max_pick on the time complexity."}
par(mfrow = c(2, 2))

plot(NULL, xlim = c(0, 1), ylim = c(0, 1),
    xlab = "Numbers of random terms, scaled", ylab = "runtime, scaled",
    main = "max_pick = 5000")
for(i in seq_along(lt)) {
    x = lt[[i]]$k
    y = lt[[i]]$t
    l = x <= 5000
    x = x[l]
    y = y[l]
    x = x/max(x)
    y = y/max(y)
    lines(x, y, col = "#00000080")
}
abline(a = 0, b = 1, col = 2, lty = 2)
curve(x^2, from = 0, to = 1, col = 3, lty = 2, add = TRUE)
curve(x^3, from = 0, to = 1, col = 4, lty = 2, add = TRUE)
legend("topleft", lty = 2, col = 2:4, legend = c("O(n)", "O(n^2)", "O(n^3)"))

df$rel_diff = sapply(lt, rel_diff, max_x = 5000)
plot(df$n_terms, df$rel_diff, log = "x",
    xlab = "Number of terms", 
    ylab = "Relative difference to linear time complexity",
    main = "max_pick = 5000")
abline(h = 0, lty = 2, col = 2)
abline(h = 1/6, lty = 2, col = 3)
abline(h = 1/4, lty = 2, col = 4)
l = df$n_terms > 100000
text(df$n_terms[l], df$rel_diff[l], df$id[l], 
    adj = c(1, -0.4), col = "blue", cex = 0.8)
legend("topright", lty = 2, col = 2:4, legend = c("O(n)", "O(n^2)", "O(n^3)"))


plot(NULL, xlim = c(0, 1), ylim = c(0, 1),
    xlab = "Numbers of random terms, scaled", ylab = "runtime, scaled",
    main = "max_pick = 1000")
for(i in seq_along(lt)) {
    x = lt[[i]]$k
    y = lt[[i]]$t
    l = x <= 1000
    x = x[l]
    y = y[l]
    x = x/max(x)
    y = y/max(y)
    lines(x, y, col = "#00000080")
}
abline(a = 0, b = 1, col = 2, lty = 2)
curve(x^2, from = 0, to = 1, col = 3, lty = 2, add = TRUE)
curve(x^3, from = 0, to = 1, col = 4, lty = 2, add = TRUE)
legend("topleft", lty = 2, col = 2:4, legend = c("O(n)", "O(n^2)", "O(n^3)"))


df$rel_diff = sapply(lt, rel_diff, max_x = 1000)
plot(df$n_terms, df$rel_diff, log = "x",
    xlab = "Number of terms", 
    ylab = "Relative difference to linear time complexity",
    main = "max_pick = 1000")
abline(h = 0, lty = 2, col = 2)
abline(h = 1/6, lty = 2, col = 3)
abline(h = 1/4, lty = 2, col = 4)
l = df$n_terms > 100000
text(df$n_terms[l], df$rel_diff[l], df$id[l], 
    adj = c(1, -0.4), col = "blue", cex = 0.8)
legend("topright", lty = 2, col = 2:4, legend = c("O(n)", "O(n^2)", "O(n^3)"))
```

### On the size of ontologies 

In the comparison we performed previously, we fix the ontology and check
the runtime performance when increasing the number of terms used for calculating similarities.
Next we do in another dimension. We fix the number of random terms and we check
the runtime performance on the size of the ontology.

We define the following `get_t_by_k()` function. For each ontology, the
runtime $t$ of a given number of terms denoted as $k$ is predicted by a loess fit.


```{r}
get_t_by_k = function(k) {
    sapply(lt, function(df) {
        i0 = which(df$k == k)
        if(length(i0)) {
            df$t[i0]
        } else {
            ind1 = which(df$k < k)
            if(length(ind1) == 0) { # k is smaller than all df$k
                return(NA)
            }
            i1 = max(ind1)
            ind2 = which(df$k > k)
            if(length(ind2) == 0) { # k is larger than all df$k
                return(NA)
            }
            i2 = min(ind2)

            x = c(0, df$k)
            y = c(0, df$t)
            fit = loess(y ~ x, span = 0.5)
            predict(fit, k)
         
        }
    })
}
```

For example, we can estimate, if we randomly sample 500 terms for calculating
similarities, the different runtime performance on ontologies with different
sizes:

```{r, fig.cap = "Figure S2-8. Runtime preformance on the size of ontologies. The number of terms for calculating semantic similarities is fixed to 500."}
t500 = get_t_by_k(500)
plot(df$n_terms, t500, log = "xy",
    xlab = "Size of the ontology", ylab = "Runtime (sec)", 
    main = "Randomly sample 500 terms from each ontology")
```

In a double-log coordinate system, the runtime has a linear
relation to the size of the ontology. We can perform a simple linear
regression:

```{r}
lm(log(t500) ~ log(df$n_terms))
```


```{r, echo = FALSE}
fit = lm(log(t500) ~ log(df$n_terms))
coef = round(coef(fit), 2)
```

Let's denote the runtime as $t$ and the size of the ontology as $N$, then they have a
linear relation: $\log(t) = `r coef[2]`*\log(N) - `r -coef[1]`$. The slope is
less than one, which means, if the ontology size increases by 2x, the runtime
only increases by $2^{`r coef[2]`}$ (`r round(2^coef[2], 2)`) times, thus the time complexity
at $k = 500$ is $O(N^{0.77})$.


We can measure the relation between runtime $t$ and ontology size $N$ for a list of different
$k$. The next function `compare_runtime_by_k()` calculates the runtime, makes the plot and 
performs linear regression for a given $k$. It returns the slope from the linear regression model.

```{r}
library(GetoptLong)
compare_runtime_by_k = function(k) {
    tt = get_t_by_k(k)
    plot(df$n_terms, tt, log = "xy",
        xlab = "N / Size of the ontology", ylab = "t / Runtime (sec)", 
        main = qq("Randomly sample @{k} terms from each ontology"))
    fit = lm(log(tt) ~ log(df$n_terms))
    coef = round(coef(fit), 2)
    coef[1] = ifelse(sign(coef[1]) == 1, paste0("+", coef[1]), coef[1])
    text(min(df$n_terms), max(tt, na.rm = TRUE), 
        qq("log(t) = @{coef[2]}*log(N)@{coef[1]}"), adj = c(0, 1))
    coef(fit)[2]
}
```

The list of $k$ we will compare is defined as follows.

```{r}
k = c(500, 600, 700, 800, 900, 1000, 2000, 3000, 
      4000, 5000, 6000, 7000, 8000, 9000, 10000)
```

We make the plot for every $k$:

```{r, animation.hook="gifski", fig.cap = "Figure S2-9. Runtime preformance on the size of ontologies, on a list of different k."}
coef = sapply(k, compare_runtime_by_k)
```

The slope coeffcient from the linear regression model measures the increasing rate of runtime
when increasing the size of the ontology. We can check how fast the runtime increases when randomly
sample different numbers of terms.

```{r, fig.cap = "Figure S2-10. The increasing rate of runtime when increase the ontology size at different k."}
plot(k, coef, xlab = "Numbers of random terms", ylab = "coef",
    main = "Runtime complexity on different k")
```

It shows when the number of query terms increases, the time complexity becomes stably around $O(N^{0.5})$.

## Algorithm

First we describe the data structures:

1. All terms are represented in integer indices: `c(1, 2, 3, ..., n_all)`.
2. The parent/child relations are represented as two lists `lt_parents` and
   `lt_children`. `lt_parents[[i]]` is a vector of integer indices of its
   parent terms. E.g. if `lt_parents[[6]] = c(3, 4, 5)`, it means term 3, 4, 5
   are parents of term 6. If you want to see term 3's parents, directly go to
   `lt_parents[[3]]`. The structure is the same for `lt_children` where
   `lt_children[[i]]` is a vector of integer indicies of its child terms. Both
   `lt_parents` and `lt_children` have length of `n_all`. If the vector of
   `lt_parents[[i]]` is empty, it means term `i` is the root. If the vector of
   `lt_children[[i]]` is empty, it means term `i` is a leaf term.
3. In the following code, a variable with name starting with `l_` is a logical
   vector with length of `n_all`. E.g. a vector `l_all_ancestors` stores
   whether term `i` is an ancestor term. Also to simplify the description of the
   algorithm, we take the `l_*` vectors as pointers where a value change in place 
   affects everywhere `l_*` variables are used.


As an example, we describe the algorithm for looking for LCA terms. The algorithm contains two steps:

**Input**: a vector of terms, already converted to integer indices.
**Output**: A matrix of depth of LCA terms.

**Step 1**: find all ancestors of the given group of terms, including the
terms themselves. They are union of ancestors of individual terms. We need to
define three functions in this step.

```r
# nodes: an integer vector
ancestors_of_a_group = function(nodes) {
    l_all_ancestors = rep(FALSE, n_terms)
    for(node in nodes) {
        l_all_ancestor[node] = TRUE  # include the nodes themselves
        # it is the same as using `add_parents`, but we want to make it clear
        # that this is the union of ancestors of individual nodes
        add_ancestors(node, l_all_ancestor)
    }
    return(l_all_ancestor)
}

# node: a single integer scalar
# l_all_ancestor: a logical vector with length n_all (let's treat it as a pointer)
add_ancestors = function(node, l_all_ancestor) {
    add_parents(node, l_all_ancestor)
}

# node: a single integer scalar
# l_all_ancestor: a logical vector with length n_all (let's treat it as a pointer)
add_parents = function(node, l_all_ancestor) {
    parents = lt_parents[[node]] # a vector of integer indicies
    if(length(parents) > 0) {
        l_all_ancestor[parents] = TRUE
        for(p in parents) {
            add_parents(node, l_all_ancestor)
        }
    }
}

l_all_ancestors = ancestors_of_a_group(nodes)
all_ancestors = which(l_all_ancestors)
```

Looking for ancestors is applied recursively by `add_parents()`. Note again, we treat
`l_all_ancestor` as a pointer so that it can be updated in the recursive parent lookup.

Step 1 generates two variables `l_all_ancestors` which is a logical vector
representing whether term `i` is an ancestor term and `all_ancestors` which is
the corresponding integer indices in `l_all_ancestors` of the `TRUE` values.

**Step 2**: All the common ancestors of `nodes` only exist in the set of
`l_all_ancestors` or `all_ancestors`. We look for LCA in a top-down manner.

For each ancestor term `an` in `all_ancestors`, we first get its offspring
terms but restricted in the background set `l_all_ancestors`. For a pair of
terms in the offspring set, if both of them are also in `nodes`, the ancestor
`an` is a common ancestor of the two terms. Then we compare the depth of `ca`
and depth previously saved for the two offspring terms. We update it to the
depth of `ca` if it is larger than the previous one.

Similarly, we need to define functions to look for offsprings, but this time
we add another constrains of the backgroud. The implemntation is very similar as
`add_ancestors()`.

```r
# nodes: an integer vector
# l_background: a logical vector with length n_all
offspring_within_background = function(node, l_background) {
    l_all_offspring = rep(FALSE, n_terms)
    l_all_offspring[node] = TRUE # include the term itself
    add_children(node, l_all_offspring, l_background)
}

# node: a single integer scalar
# l_all_offspring: a logical vector with length n_all (a pointer)
# l_background: a logical vector with length n_all
add_children = function(node, l_all_offspring, l_background) {
    children = lt_children[[node]] # a vector of integer indicies
    if(length(children) > 0) {
        l_all_offspring[children] = TRUE
        for(ch in children) {
            if(l_background[ch]) { # only continue if the child is in the background
                add_children(node, l_all_offspring, l_background)
            }
        }
    }
}
```

We pre-compute two variables. `l_nodes` corresponds to `nodes` but is a
logical vector where the i<sup>th</sup> element is `TRUE` if term `i` is in `nodes`.
`all_depth` is the depth of all terms in the ontology.

`LCA_depth` is the matrix that stores the depth of LCA terms. The initial values are set to zero.

```r
n = length(nodes)
LCA_depth = matrix(0, n, n)
diag(LCA_depth) = all_depth[nodes]

for(an in all_ancestors) {
    l_offspring = offspring_within_background(an, l_all_ancestors)
    offspring = which(l_offspring)
    no = length(offspring)

    for(i in 1:(no-1)) {
        if(l_nodes[ offspring[i] ]) {
            for(j in (i+1):no) {
                if(l_nodes[ offspring[j] ]) {
                    if(LCA_depth[i, j] < all_depth[an]) {
                        LCA_depth[i, j] = all_depth[an]
                        LCA_depth[j, i] = LCA_depth[i, j]
                    }
                }
            }
        }
    }
}
```

In this three levels of for-loop, in the triple (`ca`, `offspring[i]`,
`offspring[j]`), `an` is always an common ancestor of `offspring[i]`,
`offspring[j]`.


We can give an approximate estimation on the time complexity of the algorithm.
Let's denote set $A$ as the set of query terms and $C$ and the set of
union of all ancestors of terms in $A$. Let's denote $a_1$ and $a_2$ are the two terms in $A$,
and $c$ is a common ancestor term of $a_1$ and $a_2$.
The process involes looking for the triple $(c, a_1, a_2)$. In the algorithm
we proposed, the relations in the triple are always visited once, while
in some other tools, some relations are duplicated visited.

Let's denote the number of ancestors are $N_\mathrm{an}$ and the number of query terms are $n$,
then the approximate time complexity is $O(N_\mathrm{an}*n^2)$.

Since ancestors can be shared, $N_\mathrm{an} < n*d$ where $d$ is the average
depth of $n$ query terms. When $n$ is relatively larger, or there are more ancestors that are shared, 
$N_\mathrm{an} \ll n*d$, in this way,
the time complexity is between $O(n^2)$ and $O(n^3)$ and more closer to $O(n^2)$.


```{r, echo = FALSE}
lt = list(structure(list(k = c(100, 181, 263, 344, 426, 507, 589,
670, 752, 833, 915, 996, 1078, 1159, 1241, 1322, 1404, 1486,
1567, 1649, 1730, 1812, 1893, 1975, 2056, 2138, 2219, 2301, 2382,
2464, 2545, 2627, 2708, 2790, 2872, 2953, 3035, 3116, 3198, 3279,
3361, 3442, 3524, 3605, 3687, 3768, 3850, 3931, 4013, 4095),
    t = c(0.0199999999999818, 0.0319999999999254, 0.0440000000000964,
    0.0570000000000164, 0.0579999999999927, 0.0609999999999218,
    0.0679999999999836, 0.0819999999998799, 0.0979999999999563,
    0.0970000000002074, 0.132000000000062, 0.166999999999916,
    0.269000000000005, 0.288999999999987, 0.233999999999924,
    0.223999999999933, 0.271999999999935, 0.404999999999973,
    0.430000000000064, 0.460000000000036, 0.492999999999938,
    0.526000000000067, 0.569999999999936, 0.598999999999933,
    0.628999999999905, 0.676000000000158, 0.713999999999942,
    0.824000000000069, 0.865000000000009, 0.826000000000022,
    0.921999999999798, 0.951000000000022, 1.02600000000007, 1.06400000000008,
    1.10899999999992, 1.25, 1.1690000000001, 1.21100000000001,
    1.33699999999999, 1.35599999999999, 1.47799999999984, 1.63799999999992,
    1.60899999999992, 1.64900000000011, 1.71900000000005, 1.81700000000001,
    1.87199999999984, 1.93399999999997, 2.05100000000016, 2.21499999999992
    )), class = "data.frame", row.names = c(NA, -50L)), structure(list(
    k = c(200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000,
    2200, 2400, 2600, 2800, 3000, 3200, 3400, 3600, 3800), t = c(0.285000000000082,
    0.450000000000045, 0.605999999999995, 0.657999999999902,
    0.891999999999825, 1.08900000000017, 1.33600000000001, 1.7360000000001,
    2.154, 2.77999999999997, 3.97400000000016, 4.74900000000002,
    5.94499999999994, 6.19700000000012, 6.25800000000004, 7.60000000000014,
    8.56899999999996, 9.19999999999982, 10.4279999999999)), class = "data.frame", row.names = c(NA,
-19L)))
```

We can validated it with two DAGs. The first one `binary_tree` is a tree where each term has two children
and all leaf terms have the same depth of 11.

```{r, eval = FALSE}
lt = list()
binary_tree = dag_random(n_children = 2, tree = TRUE, depth = 11)
lt[[1]] = benchmark_runtime(binary_tree)
```

In the second DAG, each term has the number of children ranging between 3 to 10. On each term,
there is a probability of 0.8 that it connects to other terms lower in the DAG, with the number of 
terms to be connected ranging between 1 and 15. The maximal number of terms in the DAG is set to 4000.

```{r, eval = FALSE}
set.seed(37)
random_dag = dag_random(n_children = c(3, 10), max = 4000, p = 0.8, power = -1, np = c(1, 15))
lt[[2]] = benchmark_runtime(random_dag)
```

```{r, echo = FALSE}
binary_tree = dag_random(n_children = 2, tree = TRUE, depth = 11)
set.seed(37)
random_dag = dag_random(n_children = c(3, 10), max = 4000, p = 0.8, power = -1, np = c(1, 15))
```

```{r}
binary_tree
random_dag
```

In `binary_tree`, average number of parents is 1 (exclude the root), and in `random_dag`, the average
number of parents is higher, more DAG is more densely connected in `random_dag`:

```{r}
mean(n_parents(random_dag))
```

Let's visualize these two DAGs:

```{r, fig.width = 12, fig.height = 5, fig.cap = "Figure S2-11. Visualization of the binary tree and a random dense DAG."}
library(grid)
pushViewport(viewport(x = 0.25, width = 0.5))
dag_circular_viz(binary_tree, newpage = FALSE)
popViewport()

pushViewport(viewport(x = 0.75, width = 0.5))
dag_circular_viz(random_dag, edge_transparency = 0.96, newpage = FALSE)
popViewport()
```

And we compare the runtime performance on the two DAGs with different forms.

```{r, fig.width = 12, fig.height = 6, fig.cap = "Figure S2-12. Runtime performance on the binary tree and the random dense DAG. Left: absolute time complexity. Right: relative time complexity."}
par(mfrow = c(1, 2))
plot(NULL, xlim = c(0, 4100), ylim = c(0, 11),
    xlab = "Numbers of random terms", ylab = "runtime (sec)")
for(i in seq_along(lt)) {
    x = lt[[i]]$k
    y = lt[[i]]$t
    lines(x, y, col = i + 1)
}
legend("topleft", lty = 2, col = 2:3, legend = c("binary_tree", "random_dag"))

plot(NULL, xlim = c(0, 1), ylim = c(0, 1),
    xlab = "Numbers of random terms, scaled", ylab = "runtime, scaled")
for(i in seq_along(lt)) {
    x = lt[[i]]$k
    y = lt[[i]]$t
    x = x/max(x)
    y = y/max(y)
    lines(x, y, col = i + 1)
}
curve(x^1, from = 0, to = 1, lty = 2, add = TRUE)
curve(x^2, from = 0, to = 1, lty = 2, add = TRUE)
curve(x^3, from = 0, to = 1, lty = 2, add = TRUE)
legend("topleft", lty = 2, col = 2:3, legend = c("binary_tree", "random_dag"))
```

Last, inside the second `if` block in the pseudo code, the code can be generalized, such as for finding
MICA or calculating pairwise distance in the ontology, thus this agorithm is a general algorithm for
dealing with relations of term pairs on the DAG.

## Session info

```{r}
sessionInfo()
```


<script src="jquery.sticky.js"></script>
<script>
$(document).ready(function(){
    $("#TOC").sticky({
        topSpacing: 0,
        zIndex:1000    
    })
    $("#TOC").on("sticky-start", function() {

        $("<p style='font-size:1.2em; padding-left:4px;'><a id='TOC-click'>Table of Content</a></p>").insertBefore($("#TOC ul:first-child"));
        $("#TOC-click").hover(function() {
            $(this).css("color", "#0033dd").css("cursor", "pointer");
            $("#TOC").children().first().next().show();
            $("#TOC").hover(function() {
                $(this).children().first().next().show();
            }, function() {
                $(this).children().first().next().hide();
                $("body").off("hover", "#TOC");
            })
        }, function() {
            $(this).css("color", "#0033dd");
        })
        $("#TOC").children().first().next().hide();

    })
    $("#TOC").on("sticky-end", function() {
        $("#TOC").children().first().remove();
        $("#TOC").children().first().show();
    })
});
</script>
